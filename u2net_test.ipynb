{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"u2net_test.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1wm3JR2cRQDpry1KTG7-TQSr-5dWvIy0N","authorship_tag":"ABX9TyMC+wmIxYazrDvwnUNtB4jN"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1pDUbOwAc4LGmen5FUNifv2VYcuX0aznH"},"id":"Eqz6I6NzIJkl","executionInfo":{"status":"ok","timestamp":1609961251880,"user_tz":300,"elapsed":78632,"user":{"displayName":"Ian Nielsen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiolYPjdBU2iCbqmr9DVP_CKEMmLKSiMrF8i1xqcw=s64","userId":"06530265406990005395"}},"outputId":"63dd559e-5164-43bd-e276-8f8e84d02a1d"},"source":["%matplotlib inline\r\n","import os\r\n","from skimage import io, transform\r\n","import torch\r\n","import torchvision\r\n","from torch.autograd import Variable\r\n","import torch.nn as nn\r\n","import torch.nn.functional as F\r\n","from torch.utils.data import Dataset, DataLoader\r\n","from torchvision import transforms#, utils\r\n","import matplotlib.pyplot as plt\r\n","# import torch.optim as optim\r\n","\r\n","import numpy as np\r\n","from PIL import Image\r\n","import glob\r\n","\r\n","from data_loader import RescaleT\r\n","from data_loader import ToTensor\r\n","from data_loader import ToTensorLab\r\n","from data_loader import SalObjDataset\r\n","\r\n","from model import U2NET # full size version 173.6 MB\r\n","from model import U2NETP # small version u2net 4.7 MB\r\n","\r\n","# normalize the predicted SOD probability map\r\n","def normPRED(d):\r\n","    ma = torch.max(d)\r\n","    mi = torch.min(d)\r\n","\r\n","    dn = (d-mi)/(ma-mi)\r\n","\r\n","    return dn\r\n","\r\n","def save_output(image_name,pred,d_dir):\r\n","\r\n","    predict = pred\r\n","    predict = predict.squeeze()\r\n","    predict_np = predict.cpu().data.numpy()\r\n","\r\n","    im = Image.fromarray(predict_np*255).convert('RGB')\r\n","    img_name = image_name.split(os.sep)[-1]\r\n","    image = io.imread(image_name)\r\n","    imo = im.resize((image.shape[1],image.shape[0]),resample=Image.BILINEAR)\r\n","\r\n","    pb_np = np.array(imo)\r\n","\r\n","    aaa = img_name.split(\".\")\r\n","    bbb = aaa[0:-1]\r\n","    imidx = bbb[0]\r\n","    for i in range(1,len(bbb)):\r\n","        imidx = imidx + \".\" + bbb[i]\r\n","\r\n","    imo.save(d_dir+imidx+'.png')\r\n","\r\n","def imshow(img):\r\n","    img = img     # unnormalize\r\n","    npimg = img.detach().numpy()\r\n","    tpimg = np.transpose(npimg, (1, 2, 0))\r\n","    plt.imshow(tpimg)\r\n","\r\n","def VisualizeImageGrayscale(image_3d):\r\n","  r\"\"\"Returns a 3D tensor as a grayscale normalized between 0 and 1 2D tensor.\r\n","  \"\"\"\r\n","  vmin = torch.min(image_3d)\r\n","  image_2d = image_3d - vmin\r\n","  vmax = torch.max(image_2d)\r\n","  return (image_2d / vmax)\r\n","\r\n","def GetSmoothedMask(\r\n","  x_value, label, stdev_spread=.15, nsamples=25,\r\n","  magnitude=True):\r\n","    x_np = x_value.cpu().numpy()\r\n","    stdev = stdev_spread * (np.max(x_np) - np.min(x_np))\r\n","    \r\n","    total_gradients = torch.tensor(np.zeros_like(x_value.cpu()))\r\n","    for i in range(nsamples):\r\n","        noise = np.random.normal(0, stdev, x_value.shape)\r\n","        x_plus_noise = x_np + noise\r\n","        x_noise_tensor = torch.tensor(x_plus_noise, dtype = torch.float32)\r\n","        \r\n","        gradient = returnGradPred(x_noise_tensor.cuda(), label)\r\n","        \r\n","        if magnitude:\r\n","            total_gradients += abs(gradient.cpu())\r\n","        else:\r\n","            total_gradients += gradient.cpu()\r\n","    \r\n","    return total_gradients / nsamples\r\n","\r\n","bce_loss = nn.BCELoss(size_average=True, reduce = False)\r\n","\r\n","def muti_bce_loss_fusion(d0, d1, d2, d3, d4, d5, d6, labels_v):\r\n","\r\n","    loss0 = bce_loss(d0,labels_v)\r\n","    loss1 = bce_loss(d1,labels_v)\r\n","    loss2 = bce_loss(d2,labels_v)\r\n","    loss3 = bce_loss(d3,labels_v)\r\n","    loss4 = bce_loss(d4,labels_v)\r\n","    loss5 = bce_loss(d5,labels_v)\r\n","    loss6 = bce_loss(d6,labels_v)\r\n","\r\n","    loss = loss0 + loss1 + loss2 + loss3 + loss4 + loss5 + loss6\r\n","\r\n","    return loss0, loss\r\n","\r\n","def returnGradPred(img,seg):\r\n","\r\n","    img.requires_grad_(True)\r\n","    d0,d1,d2,d3,d4,d5,d6 = net(img)\r\n","    label = torch.tensor(seg)\r\n","    if (torch.cuda.is_available()):\r\n","        label = label.cuda()\r\n","    loss2, loss = muti_bce_loss_fusion(d0, d1, d2, d3, d4, d5, d6, label)\r\n","    \r\n","    loss_dot = torch.tensordot(loss, seg, dims=2)\r\n","    loss_d2 = torch.mean(loss_dot)\r\n","    # print(loss_dot.shape)\r\n","    # print(loss_dot[0][0][0][0])\r\n","    # loss_dot[0][0][0][0].backward()\r\n","    \r\n","    # sum_grad = 0\r\n","    # for i, loss_d2 in enumerate(loss_dot[0][0]):\r\n","    #   for j, l in enumerate(loss_d2):\r\n","    #     l.backward(retain_graph=True)\r\n","    #     Sc_dx = img.grad\r\n","    #     sum_grad += Sc_dx\r\n","\r\n","    loss_d2.backward()\r\n","    \r\n","    Sc_dx = img.grad\r\n","\r\n","    return Sc_dx#, pred\r\n","\r\n","if __name__ == \"__main__\":\r\n","    # --------- 1. get image path and name ---------\r\n","    model_name='u2netp'#u2net\r\n","\r\n","\r\n","\r\n","    image_dir = os.path.join(os.getcwd(), 'test_data', 'test_images')\r\n","    prediction_dir = os.path.join(os.getcwd(), 'test_data', model_name + '_results' + os.sep)\r\n","    model_dir = os.path.join(os.getcwd(), 'saved_models', model_name, model_name + '.pth')\r\n","\r\n","    img_name_list = glob.glob(image_dir + os.sep + '*')\r\n","    print(img_name_list)\r\n","\r\n","    # --------- 2. dataloader ---------\r\n","    #1. dataloader\r\n","    test_salobj_dataset = SalObjDataset(img_name_list = img_name_list,\r\n","                                        lbl_name_list = [],\r\n","                                        transform=transforms.Compose([RescaleT(320),\r\n","                                                                      ToTensorLab(flag=0)])\r\n","                                        )\r\n","    test_salobj_dataloader = DataLoader(test_salobj_dataset,\r\n","                                        batch_size=1,\r\n","                                        shuffle=False,\r\n","                                        num_workers=1)\r\n","\r\n","    # --------- 3. model define ---------\r\n","    if(model_name=='u2net'):\r\n","        print(\"...load U2NET---173.6 MB\")\r\n","        net = U2NET(3,1)\r\n","    elif(model_name=='u2netp'):\r\n","        print(\"...load U2NEP---4.7 MB\")\r\n","        net = U2NETP(3,1)\r\n","    criterion = torch.nn.CrossEntropyLoss()\r\n","    net.load_state_dict(torch.load(model_dir))\r\n","    if torch.cuda.is_available():\r\n","        net.cuda()\r\n","    net.eval()\r\n","\r\n","    # --------- 4. inference for each image ---------\r\n","    for i_test, data_test in enumerate(test_salobj_dataloader):\r\n","\r\n","        print(\"inferencing:\",img_name_list[i_test].split(os.sep)[-1])\r\n","\r\n","        inputs_test = data_test['image']\r\n","        inputs_test = inputs_test.type(torch.FloatTensor)\r\n","\r\n","        label_test = data_test['label']\r\n","        label_test = label_test.type(torch.FloatTensor)\r\n","        print(label_test.shape)\r\n","\r\n","        if torch.cuda.is_available():\r\n","            inputs_test = Variable(inputs_test.cuda())\r\n","        else:\r\n","            inputs_test = Variable(inputs_test)\r\n","\r\n","        d1,d2,d3,d4,d5,d6,d7= net(inputs_test)\r\n","\r\n","        # normalization\r\n","        pred = d1[:,0,:,:]\r\n","        pred = normPRED(pred)\r\n","        # inputs_test_gray = torch.tensor([inputs_test[:,0,:,:]\r\n","        #                                  +inputs_test[:,1,:,:]\r\n","        #                                  +inputs_test[:,2,:,:]])\r\n","        \r\n","        #phi_c = sum(pred)/(pred.shape[1]*pred.shape[2])\r\n","        #phi_c = torch.tensor([[phi_c.clone().detach().cpu().numpy()]]).cuda()\r\n","        pred_ = torch.tensor([pred.clone().detach().cpu().numpy()]).cuda()\r\n","\r\n","        #print(phi_c.shape)\r\n","        \r\n","        # save results to test_results folder\r\n","        if not os.path.exists(prediction_dir):\r\n","            os.makedirs(prediction_dir, exist_ok=True)\r\n","        save_output(img_name_list[i_test],pred,prediction_dir)\r\n","        grad_map = returnGradPred(inputs_test.clone().detach(),pred_)\r\n","        vanilla_grad = grad_map.clone().detach().cpu()\r\n","        vanilla_grad_sq = abs(vanilla_grad)\r\n","        smoothgrad = GetSmoothedMask(inputs_test.clone().detach(),pred_,magnitude=False)\r\n","        smoothgrad_sq = GetSmoothedMask(inputs_test.clone().detach(),pred_,magnitude=True)\r\n","\r\n","        fig=plt.figure(figsize=(30, 20))\r\n","        length, width = 1, 4\r\n","        fig.add_subplot(length, width, 1)\r\n","        imshow(torchvision.utils.make_grid(VisualizeImageGrayscale(inputs_test.clone().detach().cpu())))\r\n","        plt.axis('off')\r\n","        fig.add_subplot(length, width, 2)\r\n","        imshow(torchvision.utils.make_grid(VisualizeImageGrayscale(pred.clone().detach().cpu())))\r\n","        plt.axis('off')\r\n","        fig.add_subplot(length, width, 3)\r\n","        imshow(torchvision.utils.make_grid(VisualizeImageGrayscale(smoothgrad)))\r\n","        plt.axis('off')\r\n","        fig.add_subplot(length, width, 4)\r\n","        imshow(torchvision.utils.make_grid(VisualizeImageGrayscale(smoothgrad_sq)))\r\n","        plt.axis('off')\r\n","\r\n","        # imshow(torchvision.utils.make_grid(VisualizeImageGrayscale(phi_c.cpu())))\r\n","        # plt.axis('off')\r\n","        plt.show()\r\n","        #save_output(img_name_list[i_test]+'_grad',grad_map,prediction_dir)\r\n","\r\n","        del d1,d2,d3,d4,d5,d6,d7"],"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lmi7Jy8ZVJAh","executionInfo":{"status":"ok","timestamp":1609812998393,"user_tz":300,"elapsed":394,"user":{"displayName":"Ian Nielsen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiolYPjdBU2iCbqmr9DVP_CKEMmLKSiMrF8i1xqcw=s64","userId":"06530265406990005395"}},"outputId":"4e0ec15a-91bd-48b0-e2e2-1f2ec8aa09fc"},"source":["pred"],"execution_count":47,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[0.0038, 0.0017, 0.0006,  ..., 0.0017, 0.0007, 0.0013],\n","         [0.0015, 0.0004, 0.0001,  ..., 0.0007, 0.0002, 0.0003],\n","         [0.0006, 0.0002, 0.0001,  ..., 0.0005, 0.0003, 0.0002],\n","         ...,\n","         [0.0004, 0.0002, 0.0001,  ..., 0.0005, 0.0003, 0.0008],\n","         [0.0005, 0.0001, 0.0001,  ..., 0.0007, 0.0005, 0.0008],\n","         [0.0012, 0.0001, 0.0003,  ..., 0.0015, 0.0008, 0.0040]]],\n","       device='cuda:0', grad_fn=<DivBackward0>)"]},"metadata":{"tags":[]},"execution_count":47}]},{"cell_type":"markdown","metadata":{"id":"EZX14ZzEO5CI"},"source":["Creates a criterion that measures the Binary Cross Entropy\r\n","between the target and the output:\r\n","\r\n","The unreduced (i.e. with reduction set to 'none') loss can be described as:\r\n","\r\n","where N is the batch size. If reduction is not 'none'\r\n","(default 'mean'), then\r\n","\r\n","This is used for measuring the error of a reconstruction in for example\r\n","an auto-encoder. Note that the targets y should be numbers\r\n","between 0 and 1.\r\n","\r\n","Notice that if x_n is either 0 or 1, one of the log terms would be\r\n","mathematically undefined in the above loss equation. PyTorch chooses to set\r\n","\\log (0) = -\\infty, since \\lim_{x\\to 0} \\log (x) = -\\infty.\r\n","However, an infinite term in the loss equation is not desirable for several reasons.\r\n","\r\n","For one, if either y_n = 0 or (1 - y_n) = 0, then we would be\r\n","multiplying 0 with infinity. Secondly, if we have an infinite loss value, then\r\n","we would also have an infinite term in our gradient, since\r\n","\\lim_{x\\to 0} \\frac{d}{dx} \\log (x) = \\infty.\r\n","This would make BCELoss's backward method nonlinear with respect to x_n,\r\n","and using it for things like linear regression would not be straight-forward.\r\n","\r\n","Our solution is that BCELoss clamps its log function outputs to be greater than\r\n","or equal to -100. This way, we can always have a finite loss value and a linear\r\n","backward method.\r\n","\r\n","Args:\r\n","    weight (Tensor, optional): a manual rescaling weight given to the loss\r\n","        of each batch element. If given, has to be a Tensor of size nbatch.\r\n","    size_average (bool, optional): Deprecated (see reduction). By default,\r\n","        the losses are averaged over each loss element in the batch. Note that for\r\n","        some losses, there are multiple elements per sample. If the field size_average\r\n","        is set to False, the losses are instead summed for each minibatch. Ignored\r\n","        when reduce is False. Default: True\r\n","    reduce (bool, optional): Deprecated (see reduction). By default, the\r\n","        losses are averaged or summed over observations for each minibatch depending\r\n","        on size_average. When reduce is False, returns a loss per\r\n","        batch element instead and ignores size_average. Default: True\r\n","    reduction (string, optional): Specifies the reduction to apply to the output:\r\n","        'none' | 'mean' | 'sum'. 'none': no reduction will be applied,\r\n","        'mean': the sum of the output will be divided by the number of\r\n","        elements in the output, 'sum': the output will be summed. Note: size_average\r\n","        and reduce are in the process of being deprecated, and in the meantime,\r\n","        specifying either of those two args will override reduction. Default: 'mean'\r\n","\r\n","Shape:\r\n","    - Input: (N, *) where * means, any number of additional\r\n","      dimensions\r\n","    - Target: (N, *), same shape as the input\r\n","    - Output: scalar. If reduction is 'none', then (N, *), same\r\n","      shape as input.\r\n","\r\n","Examples:\r\n",">>> m = nn.Sigmoid()\r\n",">>> loss = nn.BCELoss()\r\n",">>> input = torch.randn(3, requires_grad=True)\r\n",">>> target = torch.empty(3).random_(2)\r\n",">>> output = loss(m(input), target)\r\n",">>> output.backward()\r\n","\r\n","\r\n"]}]}